{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPER5+W9dbC79lUYdP6SW3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywang1110/PyTorch_Colab_Files/blob/main/01_02_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto Grad"
      ],
      "metadata": {
        "id": "oOqfBwehmA-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ What is a gradient?\n",
        "\n",
        "A **gradient** is the **slope** or **derivative** of a function — it tells you:\n",
        "\n",
        "> **How much the output changes when the input changes.**\n",
        "\n",
        "In deep learning, it's how we know which direction to move our parameters to **minimize loss**.\n",
        "\n",
        "---\n",
        "\n",
        "**🧭 Intuition (Simple English)**\n",
        "\n",
        "Imagine you're on a hill and want to go **down** to the lowest point.\n",
        "\n",
        "* The **gradient** tells you:\n",
        "\n",
        "  * Which **direction is downhill**\n",
        "  * How **steep** the slope is\n",
        "\n",
        "That’s how **gradient descent** works!\n",
        "\n",
        "---\n",
        "\n",
        "**🔧 In PyTorch**\n",
        "\n",
        "If you have:\n",
        "\n",
        "```python\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2 + 3*x + 1\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "```\n",
        "\n",
        "PyTorch computes:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = 2x + 3\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "x.grad = 2×2 + 3 = 7\n",
        "$$\n",
        "\n",
        "This gradient tells you:\n",
        "\n",
        "> If `x` increases by a little, `y` increases by **about 7× that amount**\n",
        "\n",
        "---\n",
        "\n",
        "**📚 In Deep Learning**\n",
        "\n",
        "* The **gradient of the loss** tells the model:\n",
        "\n",
        "  > How should I adjust my weights to make predictions better?\n",
        "\n",
        "* Optimizers (like SGD, Adam) use these gradients to **update model parameters**.\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Summary**\n",
        "\n",
        "| Term       | Meaning                                        |\n",
        "| ---------- | ---------------------------------------------- |\n",
        "| Gradient   | The rate of change of a value (slope)          |\n",
        "| Use        | Tells how to adjust variables to minimize loss |\n",
        "| In PyTorch | Computed by `.backward()`, stored in `.grad`   |"
      ],
      "metadata": {
        "id": "zgsKDfHMwtq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy"
      ],
      "metadata": {
        "id": "6DdK889ZmthI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Autograd"
      ],
      "metadata": {
        "id": "nx043FLimF88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Tensor that requires gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "4PqCaqiPnJbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensor.requires_grad\n",
        "* Is True if gradients need to be computed for this Tensor,\n",
        "* False otherwise.\n"
      ],
      "metadata": {
        "id": "3oSfyjfgmZUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BixC_v1lvkx",
        "outputId": "5b374aaa-f444-43bc-a715-bd5395d81db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor: 2.0\n",
            "Requires grad: True\n"
          ]
        }
      ],
      "source": [
        "input_x = torch.tensor(2.0, requires_grad=True)\n",
        "print(f\"input tensor: {input_x}\")\n",
        "print(f\"Requires grad: {input_x.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define function y = x^2 + 3x + 1"
      ],
      "metadata": {
        "id": "sZH42_9Zneh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_y = input_x**2 + input_x*3 + 1\n",
        "print(f\"Function value: {output_y}\")\n",
        "print(f\"y requires grad: {output_y.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SodGdNunHb3",
        "outputId": "4304284b-5d4c-4e3f-cda3-0db4fe74eb8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function value: 11.0\n",
            "y requires grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute gradient"
      ],
      "metadata": {
        "id": "Pp27_Dzrn8Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### torch.Tensor.backward\n",
        "\n",
        "* It tells PyTorch to **compute the gradient of `output_y` with respect to `input_x`**\n",
        "* Since `output_y = input_x**2 + input_x*3 + 1`, the math is:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = 2x + 3\n",
        "$$\n",
        "\n",
        "At `x = 2.0`, we get:\n",
        "\n",
        "$$\n",
        "2 × 2 + 3 = 4 + 3 = 7\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "| Expression            | Meaning                          |\n",
        "| --------------------- | -------------------------------- |\n",
        "| `output_y.backward()` | Computes gradient dy/dx          |\n",
        "| `input_x.grad`        | Shows result: dy/dx at `x = 2.0` |\n",
        "| Gradient value        | `7.0`                            |\n",
        "\n"
      ],
      "metadata": {
        "id": "K8ZDwpA9o6qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_y.backward()\n",
        "print(input_x.grad) # dy/dx = 2x + 3 → at x=2 → 2*2 + 3 = 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmxsnic3ntBL",
        "outputId": "1ed675c5-9e93-4ffa-fe31-3303997f1d05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero gradients"
      ],
      "metadata": {
        "id": "OIiyYiUzp0XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_x.grad.zero_())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XZSsYo8npxl",
        "outputId": "849b935d-b273-4f4d-8bdf-9b279eef3b10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Autograd"
      ],
      "metadata": {
        "id": "x1ERhCtUm2ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create vector\n"
      ],
      "metadata": {
        "id": "aavnVbopqhE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector =torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "print(f\"Input vector: {input_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNbM6S2tpD8Z",
        "outputId": "a019cd78-4059-44eb-b49c-d9717fdd1de6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector: tensor([1., 2., 3.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector function"
      ],
      "metadata": {
        "id": "S7U1Ibpjqy0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_vector = input_vector**2 + input_vector*3 + 1 # element-wise operations\n",
        "print(f\"Function values: {output_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1cc1lcgq2bi",
        "outputId": "5afa2dac-d86b-4052-80b4-55c2d74e900d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function values: tensor([ 5., 11., 19.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scalar output for backpropagation"
      ],
      "metadata": {
        "id": "GbzZGj7mrQpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_value = output_vector.sum() # sum to get scalar\n",
        "print(f\"Loss value: {loss_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy1A_-tBrOWk",
        "outputId": "7a4e7d15-fbd4-442e-fb33-51a99d248291"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss value: 35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute gradients"
      ],
      "metadata": {
        "id": "fhA6AE1GrkiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_value.backward()\n",
        "print(f\"Gradients: {input_vector.grad}\") # y = 2*x + 3\n",
        "print(f\"Theortical values: {2*input_vector + 3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_43pHYgprnDM",
        "outputId": "44dbabfb-e451-4e4e-8c04-8921767bb2e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients: tensor([5., 7., 9.])\n",
            "Theortical values: tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational Graph"
      ],
      "metadata": {
        "id": "r_MepfWdssER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build complex computational graph"
      ],
      "metadata": {
        "id": "LeE7G6slszN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_x = torch.tensor(1.0, requires_grad=True)\n",
        "var_a = torch.tensor(2.0, requires_grad=True)\n",
        "var_b = torch.tensor(3.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "eb0JnJhbsqPt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composite function"
      ],
      "metadata": {
        "id": "VImraNqjs4Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_y = var_a * var_x**2 + var_b * var_x + 1"
      ],
      "metadata": {
        "id": "Q0hyftrWs6zS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_z = intermediate_y**2 + 2*intermediate_y"
      ],
      "metadata": {
        "id": "gRSdQ1Q-tJIg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x = {var_x.data}, a = {var_a.data}, b = {var_b.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p4dfbk3tN9H",
        "outputId": "5cdffb70-e9a4-4789-c2bc-b79b97d670d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 1.0, a = 2.0, b = 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"y = a*x^2 + b*x + 1 = {intermediate_y.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJxgodQwtVLe",
        "outputId": "224871dc-7dc7-40c3-ba88-2136096942de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = a*x^2 + b*x + 1 = 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"z = y^2 + 2*y = {final_z.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtxdufQjtZ89",
        "outputId": "5b67099d-020c-4d3b-d845-5a89d89c0ec2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z = y^2 + 2*y = 48.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpopagation"
      ],
      "metadata": {
        "id": "0-iGk4Bjtg7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_z.backward()"
      ],
      "metadata": {
        "id": "pRB8A29Itjdk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dz/dx = {var_x.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH_a8mY2tpU1",
        "outputId": "868a0a6f-9260-4965-aaa6-ac4cb2e62744"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dz/dx = 98.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dz/da = {var_a.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELKxI9E-tzTG",
        "outputId": "ddf8a1db-0399-4041-ef68-bcbffa82dedf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dz/da = 14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dz/db = {var_b.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5VFJLn5t2Mj",
        "outputId": "8ffde4ab-4a68-48cc-f09b-c99a72eae4ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dz/db = 14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Verification"
      ],
      "metadata": {
        "id": "wXQTN015uR5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🧩 Step 1: Define scalar variables with gradients**\n",
        "\n",
        "```python\n",
        "var_x = torch.tensor(1.0, requires_grad=True)\n",
        "var_a = torch.tensor(2.0, requires_grad=True)\n",
        "var_b = torch.tensor(3.0, requires_grad=True)\n",
        "```\n",
        "\n",
        "You define 3 scalar variables:\n",
        "\n",
        "* `x = 1.0`, `a = 2.0`, `b = 3.0`\n",
        "* All have `requires_grad=True` to track gradients\n",
        "\n",
        "---\n",
        "\n",
        "**🧮 Step 2: Build composite function**\n",
        "\n",
        "```python\n",
        "intermediate_y = var_a * var_x**2 + var_b * var_x + 1\n",
        "final_z = intermediate_y**2 + 2 * intermediate_y\n",
        "```\n",
        "\n",
        "This builds:\n",
        "\n",
        "### Inner function:\n",
        "\n",
        "$$\n",
        "y = a x^2 + b x + 1\n",
        "$$\n",
        "\n",
        "### Final function:\n",
        "\n",
        "$$\n",
        "z = y^2 + 2y\n",
        "$$\n",
        "\n",
        "So this is a **composite function**:\n",
        "$z = (a x^2 + b x + 1)^2 + 2(a x^2 + b x + 1)$\n",
        "\n",
        "---\n",
        "\n",
        "**📤 Step 3: Forward Pass (Print Values)**\n",
        "\n",
        "```python\n",
        "print(f\"x = {var_x.data}, a = {var_a.data}, b = {var_b.data}\")\n",
        "print(f\"y = a*x^2 + b*x + 1 = {intermediate_y.data}\")\n",
        "print(f\"z = y^2 + 2*y = {final_z.data}\")\n",
        "```\n",
        "\n",
        "At:\n",
        "\n",
        "* `x = 1`, `a = 2`, `b = 3`\n",
        "* Then:\n",
        "\n",
        "$$\n",
        "y = 2×1^2 + 3×1 + 1 = 6 \\\\\n",
        "z = 6^2 + 2×6 = 36 + 12 = 48\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**🔁 Step 4: Backward (Auto Gradients)**\n",
        "\n",
        "```python\n",
        "final_z.backward()\n",
        "```\n",
        "\n",
        "Now PyTorch computes:\n",
        "\n",
        "* $\\frac{dz}{dx}$\n",
        "* $\\frac{dz}{da}$\n",
        "* $\\frac{dz}{db}$\n",
        "\n",
        "Then you print:\n",
        "\n",
        "```python\n",
        "print(f\"dz/dx = {var_x.grad}\")\n",
        "print(f\"dz/da = {var_a.grad}\")\n",
        "print(f\"dz/db = {var_b.grad}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**📐 Step 5: Manual gradient for $dz/dx$**\n",
        "\n",
        "Using chain rule:\n",
        "\n",
        "$$\n",
        "z = y^2 + 2y \\Rightarrow \\frac{dz}{dy} = 2y + 2 \\\\\n",
        "y = ax^2 + bx + 1 \\Rightarrow \\frac{dy}{dx} = 2ax + b\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = (2y + 2)(2ax + b)\n",
        "$$\n",
        "\n",
        "```python\n",
        "manual_dz_dx = (2*intermediate_y.data + 2) * (2*var_a.data*var_x.data + var_b.data)\n",
        "```\n",
        "\n",
        "Plug in the values:\n",
        "\n",
        "* `y = 6`, `a = 2`, `x = 1`, `b = 3`\n",
        "* $$\n",
        "  $$\n",
        "\n",
        "dz/dx = (2×6 + 2)(2×2×1 + 3) = 14 × 7 = 98\n",
        "]\n",
        "\n",
        "So this confirms:\n",
        "\n",
        "```python\n",
        "dz/dx = 98\n",
        "```\n",
        "\n",
        "✅ Matches `var_x.grad`\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Summary**\n",
        "\n",
        "| Variable | Meaning               | Gradient Calculated  |\n",
        "| -------- | --------------------- | -------------------- |\n",
        "| `x`      | Input                 | $(2y + 2)(2a x + b)$ |\n",
        "| `a`      | Quadratic coefficient | $(2y + 2)(x^2)$      |\n",
        "| `b`      | Linear coefficient    | $(2y + 2)(x)$        |"
      ],
      "metadata": {
        "id": "LvR2SuUzvHb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Accumulation"
      ],
      "metadata": {
        "id": "S82caFWGxwad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create parameter"
      ],
      "metadata": {
        "id": "wyYMkW5Sx1gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_x = torch.tensor(1.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "3D0k7juruVom"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mutiple forward and backward passes"
      ],
      "metadata": {
        "id": "nfv_UaiqyDc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(3):\n",
        "  output_y = param_x**2 + iteration\n",
        "  output_y.backward()\n",
        "  print(f\"Iteration {iteration + 1} : {param_x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qddbUlP8P7u",
        "outputId": "280f42d9-24ee-42ff-9723-3fc7320c1ebc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 : 2.0\n",
            "Iteration 2 : 4.0\n",
            "Iteration 3 : 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🧩 Code Breakdown\n",
        "\n",
        "```python\n",
        "param_x = torch.tensor(1.0, requires_grad=True)\n",
        "```\n",
        "\n",
        "* You define a scalar parameter `x = 1.0`\n",
        "* `requires_grad=True` lets PyTorch track gradients\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Loop with forward and backward\n",
        "\n",
        "```python\n",
        "for iteration in range(3):\n",
        "    output_y = param_x**2 + iteration\n",
        "    output_y.backward()\n",
        "    print(f\"Iteration {iteration+1}: gradient = {param_x.grad}\")\n",
        "```\n",
        "\n",
        "Each time:\n",
        "\n",
        "$$\n",
        "y = x^2 + \\text{iteration}\n",
        "\\Rightarrow \\frac{dy}{dx} = 2x\n",
        "\\Rightarrow \\frac{dy}{dx} = 2 \\times 1.0 = 2\n",
        "$$\n",
        "\n",
        "But since **you didn’t clear gradients**, PyTorch **adds** each new gradient to the previous one.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Output:\n",
        "\n",
        "```python\n",
        "Iteration 1: gradient = 2.0\n",
        "Iteration 2: gradient = 4.0   (2 + 2)\n",
        "Iteration 3: gradient = 6.0   (4 + 2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Why gradients accumulate\n",
        "\n",
        "By default, PyTorch does **not reset `.grad`** after `.backward()`\n",
        "So gradients **accumulate** — this is useful when:\n",
        "\n",
        "* Doing custom gradient accumulation across mini-batches\n",
        "* Manually controlling gradient updates\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "| Concept                  | Meaning                                 |\n",
        "| ------------------------ | --------------------------------------- |\n",
        "| `.backward()`            | Adds gradient to `.grad`                |\n",
        "| `.grad.zero_()`          | Clears stored gradient                  |\n",
        "| Why gradients accumulate | By design (e.g. for batch accumulation) |"
      ],
      "metadata": {
        "id": "atToYg_H7-bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clear gradients"
      ],
      "metadata": {
        "id": "0rp-5ish8lgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient: {param_x.grad}\")\n",
        "param_x.grad.zero_();\n",
        "print(f\"Gradient after clearing: {param_x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6JYNwtryCuo",
        "outputId": "1ef0b7db-b926-4f23-b6a0-47314e6f1eeb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: 6.0\n",
            "Gradient after clearing: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable Gradient Demo"
      ],
      "metadata": {
        "id": "SDjDeiPp9Lj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_x = torch.tensor(1.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "O9cefRah9QMm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_y1 = input_x ** 2\n",
        "print(f\"Normal computation: requires_grad = {output_y1.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqoYhOlg9j2V",
        "outputId": "4ff78fdb-4539-42f0-8b5e-0008c8969db2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal computation: requires_grad = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use torch.no_grad() to disable gradient"
      ],
      "metadata": {
        "id": "BV2-RhA89ugI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output_y2 = input_x ** 2\n",
        "  print(f\"no_grad context: requires_grad = {output_y2.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIuLGda9r73",
        "outputId": "0f3e5bf0-ef2f-4595-fac5-f82e6b3d6f53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_grad context: requires_grad = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use detach() to detach tensor"
      ],
      "metadata": {
        "id": "xba9nO9b-ATg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_y3 = input_x ** 2\n",
        "print(f\"Before detach: requires_grad = {output_y3.requires_grad}\")\n",
        "y3_detached = output_y3.detach()\n",
        "print(f\"After detach: requires_grad = {y3_detached.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwpJUYNI9-Xi",
        "outputId": "0f001b59-a2c6-4229-a6d1-4b47a68edf8a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before detach: requires_grad = True\n",
            "After detach: requires_grad = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Autograd Example"
      ],
      "metadata": {
        "id": "w-HGdI6G-qoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "n_samples = 100\n",
        "true_weight = 2.0\n",
        "true_bias = 1.0"
      ],
      "metadata": {
        "id": "wheltjnB_CAu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_x = torch.randn(n_samples, 1)\n",
        "print(input_x.shape)\n",
        "print(input_x[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap5rrapu_OFb",
        "outputId": "d40b0447-ee61-4277-9d20-d1b0dc45ab58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1])\n",
            "tensor([[ 1.9269],\n",
            "        [ 1.4873],\n",
            "        [ 0.9007],\n",
            "        [-2.1055],\n",
            "        [ 0.6784]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_y = input_x * true_weight + true_bias + 0.1 * torch.randn(n_samples, 1)"
      ],
      "metadata": {
        "id": "UI9qeq7Z_fdt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize parameters"
      ],
      "metadata": {
        "id": "mqdcSKju-7In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_param = torch.tensor(0.0, requires_grad=True)\n",
        "bias_param = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "-AyMyO2I-v7M"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train parameters"
      ],
      "metadata": {
        "id": "ui6NiDLRATT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 100"
      ],
      "metadata": {
        "id": "5KKxq_lmAgzu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []"
      ],
      "metadata": {
        "id": "g7tV7KwSAR24"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  # Forward pass\n",
        "  predictions = weight_param * input_x + bias_param\n",
        "  loss = ((predictions - target_y)**2).mean() # MSE\n",
        "\n",
        "  # propagation / backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # paramter update\n",
        "  with torch.no_grad():\n",
        "    weight_param -= learning_rate * weight_param.grad\n",
        "    bias_param -= learning_rate * bias_param.grad\n",
        "\n",
        "    # zero gradients\n",
        "    weight_param.grad.zero_()\n",
        "    bias_param.grad.zero_()\n",
        "\n",
        "  loss_history.append(loss.item())\n",
        "\n",
        "  if ((epoch + 1) % 20 == 0):\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}, w = {weight_param.item():.4f}, b = {bias_param.item():.4f}\")\n",
        "\n",
        "print(f\"\\nFinal parameters:\")\n",
        "print(f\"Learned weight: {weight_param.item():.4f} (True: {true_weight})\")\n",
        "print(f\"Learned bias: {bias_param.item():.4f} (True: {true_bias})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQoGB6rVApKu",
        "outputId": "19bc6381-5df6-45ed-99cf-b5f81a26d0fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Loss = 2.3416, w = 0.6630, b = 0.3662\n",
            "Epoch 40: Loss = 1.0303, w = 1.1058, b = 0.5999\n",
            "Epoch 60: Loss = 0.4566, w = 1.4017, b = 0.7487\n",
            "Epoch 80: Loss = 0.2051, w = 1.5996, b = 0.8432\n",
            "Epoch 100: Loss = 0.0947, w = 1.7320, b = 0.9030\n",
            "\n",
            "Final parameters:\n",
            "Learned weight: 1.7320 (True: 2.0)\n",
            "Learned bias: 0.9030 (True: 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom function demo"
      ],
      "metadata": {
        "id": "FvdMQkMYEsQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynQz-mfNEr_N"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}